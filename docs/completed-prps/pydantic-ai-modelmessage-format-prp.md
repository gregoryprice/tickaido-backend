# PRP: Pydantic AI ModelMessage Format Implementation

## Problem Statement

The current message history implementation in `DynamicAgentFactory` is passing simple `List[Dict[str, str]]` format to the `message_history` parameter, but Pydantic AI expects the proper `ModelMessage` format with `ModelRequest` and `ModelResponse` objects. This mismatch causes:

1. **Wrong Message Format**: Simple dict format `[{'role': 'user', 'content': 'what can you do?'}]` instead of proper `ModelMessage` objects
2. **Missing ModelMessagesTypeAdapter Usage**: Not using the required TypeAdapter for proper serialization/deserialization
3. **Incomplete Conversation Context**: Pydantic AI cannot properly understand conversation structure without proper message parts
4. **Agent Run Failures**: Second messages in conversations fail because agent expects `ModelMessage` format

## Current State Analysis

### Flawed Implementation (app/services/dynamic_agent_factory.py:214)
```python
# INCORRECT - Simple dict format being passed
message_history: List[Dict[str, str]] = [
    {"role": "user", "content": "what can you do?"},
    {"role": "assistant", "content": "I can help with..."}
]

result = await pydantic_agent.run(
    message,
    message_history=message_history,  # WRONG FORMAT
    usage_limits=usage_limits
)
```

### Expected Pydantic AI Format
Based on [Pydantic AI documentation](https://ai.pydantic.dev/message-history/#using-messages-as-input-for-further-agent-runs), the correct format is:

```python
# CORRECT - ModelMessage format with proper parts
message_history: List[ModelMessage] = [
    ModelRequest(
        parts=[
            UserPromptPart(
                content='what can you do?',
                timestamp=datetime.datetime(...)
            )
        ]
    ),
    ModelResponse(
        parts=[
            TextPart(
                content='I can assist you with...'
            )
        ],
        usage=RequestUsage(input_tokens=10, output_tokens=20),
        model_name='gpt-4o-mini',
        timestamp=datetime.datetime(...)
    )
]
```

## Proposed Solution Architecture

### 1. Message Conversion Service

Create `app/services/message_converter_service.py` with proper ModelMessage conversion:

```python
from typing import List, Dict, Any
from datetime import datetime, timezone
from pydantic import TypeAdapter
from pydantic_ai.messages import (
    ModelMessage, 
    ModelRequest, 
    ModelResponse,
    UserPromptPart,
    TextPart,
    RequestUsage
)
from pydantic_core import to_jsonable_python
from app.models.chat import Message
import logging

logger = logging.getLogger(__name__)

# Create TypeAdapter for ModelMessage serialization
ModelMessagesTypeAdapter = TypeAdapter(List[ModelMessage])

class MessageConverterService:
    """Service for converting database messages to Pydantic AI ModelMessage format."""
    
    async def convert_db_messages_to_model_messages(
        self,
        db_messages: List[Message]
    ) -> List[ModelMessage]:
        """
        Convert database Message objects to Pydantic AI ModelMessage format.
        
        Args:
            db_messages: List of Message objects from database
            
        Returns:
            List[ModelMessage]: Properly formatted messages for Pydantic AI
        """
        try:
            model_messages: List[ModelMessage] = []
            
            for msg in db_messages:
                if msg.role == "user":
                    # Create ModelRequest for user messages
                    model_request = ModelRequest(
                        parts=[
                            UserPromptPart(
                                content=msg.content,
                                timestamp=msg.created_at or datetime.now(timezone.utc)
                            )
                        ]
                    )
                    model_messages.append(model_request)
                    
                elif msg.role == "assistant":
                    # Create ModelResponse for assistant messages
                    model_response = ModelResponse(
                        parts=[
                            TextPart(content=msg.content)
                        ],
                        usage=RequestUsage(
                            input_tokens=0,  # Would need to calculate or store
                            output_tokens=0  # Would need to calculate or store
                        ),
                        model_name="gpt-4o-mini",  # Default model name
                        timestamp=msg.created_at or datetime.now(timezone.utc)
                    )
                    model_messages.append(model_response)
            
            logger.info(f"Converted {len(db_messages)} messages to ModelMessage format")
            return model_messages
            
        except Exception as e:
            logger.error(f"Error converting messages to ModelMessage format: {e}")
            return []
    
    async def serialize_model_messages_to_json(
        self,
        model_messages: List[ModelMessage]
    ) -> str:
        """Serialize ModelMessages to JSON using TypeAdapter."""
        try:
            json_data = to_jsonable_python(model_messages)
            return json.dumps(json_data)
        except Exception as e:
            logger.error(f"Error serializing ModelMessages: {e}")
            return "[]"
    
    async def deserialize_json_to_model_messages(
        self,
        json_string: str
    ) -> List[ModelMessage]:
        """Deserialize JSON to ModelMessages using TypeAdapter."""
        try:
            json_data = json.loads(json_string)
            return ModelMessagesTypeAdapter.validate_python(json_data)
        except Exception as e:
            logger.error(f"Error deserializing ModelMessages: {e}")
            return []

message_converter_service = MessageConverterService()
```

### 2. Enhanced AI Chat Service

Update `ai_chat_service.py` to return proper ModelMessage format:

```python
async def get_thread_history_as_model_messages(
    self,
    thread_id: str,
    user_id: str,
    agent_id: str,
    max_context_size: Optional[int] = None,
    use_memory_context: bool = True
) -> List[ModelMessage]:
    """
    Get thread history as Pydantic AI ModelMessage format.
    
    Returns:
        List[ModelMessage]: Properly formatted messages for Pydantic AI
    """
    if not use_memory_context:
        logger.debug(f"Memory context disabled for thread {thread_id}")
        return []
    
    async with get_async_db_session() as db:
        try:
            # Verify user owns thread and agent is valid (existing logic)
            thread_query = select(Thread).where(
                Thread.id == UUID(thread_id),
                Thread.user_id == user_id,
                Thread.agent_id == UUID(agent_id)
            )
            
            result = await db.execute(thread_query)
            thread = result.scalar_one_or_none()
            
            if not thread:
                logger.warning(f"Thread {thread_id} not found for user {user_id} and agent {agent_id}")
                return []
            
            # Get messages in chronological order
            messages_query = select(Message).where(
                Message.thread_id == UUID(thread_id)
            ).order_by(Message.created_at)
            
            result = await db.execute(messages_query)
            db_messages = result.scalars().all()
            
            # Apply context limits if specified
            if max_context_size and max_context_size > 0:
                db_messages = await self._apply_context_limits(db_messages, max_context_size)
            
            # Convert to ModelMessage format
            from app.services.message_converter_service import message_converter_service
            model_messages = await message_converter_service.convert_db_messages_to_model_messages(db_messages)
            
            logger.info(f"Retrieved {len(model_messages)} ModelMessages for Pydantic AI agent")
            return model_messages
            
        except Exception as e:
            logger.error(f"Error getting ModelMessages for agent: {e}")
            return []
```

### 3. Updated Dynamic Agent Factory

Update `dynamic_agent_factory.py` to use proper ModelMessage format:

```python
async def process_message_with_agent(
    self,
    agent_model: AgentModel,
    message: str,
    context: CustomerSupportContext,
    auth_token: Optional[str] = None,
    thread_id: Optional[str] = None,
    db: Optional[AsyncSession] = None
) -> ChatResponse:
    """Process message with proper Pydantic AI ModelMessage history integration."""
    try:
        # Create Pydantic AI agent
        pydantic_agent = await self.create_agent_from_model(
            agent_model, 
            auth_token=auth_token
        )
        
        if not pydantic_agent:
            logger.error(f"Failed to create agent from model {agent_model.id}")
            return self._create_error_response()
        
        # Retrieve message history in proper ModelMessage format
        message_history: List[ModelMessage] = []
        if thread_id and agent_model.use_memory_context:
            try:
                from app.services.ai_chat_service import ai_chat_service
                
                # Get user info from context if available
                user_id = context.user_metadata.get('user_id', 'system')
                
                # Use new method that returns ModelMessage format
                message_history = await ai_chat_service.get_thread_history_as_model_messages(
                    thread_id=thread_id,
                    user_id=user_id,
                    agent_id=str(agent_model.id),
                    max_context_size=agent_model.max_context_size,
                    use_memory_context=agent_model.use_memory_context
                )
                
                logger.info(
                    f"Loaded {len(message_history)} ModelMessages for context "
                    f"(thread: {thread_id})"
                )
            except Exception as e:
                logger.error(f"Failed to load ModelMessages for thread {thread_id}: {e}")
                message_history = []
        
        # Configure usage limits
        usage_limits = UsageLimits(request_limit=agent_model.max_iterations)
        
        # CORRECT PYDANTIC AI USAGE - Use ModelMessage format
        if message_history:
            # Run agent with proper ModelMessage history
            result = await pydantic_agent.run(
                message,
                message_history=message_history,  # Now in correct ModelMessage format
                usage_limits=usage_limits
            )
        else:
            # Run agent without history
            result = await pydantic_agent.run(
                message,
                usage_limits=usage_limits
            )
        
        # Extract response
        response = result.output if hasattr(result, 'output') else result
        tools_used = getattr(result, 'tools_used', []) or []
        
        # Store conversation result for future history
        if thread_id:
            asyncio.create_task(
                self._store_conversation_result(
                    thread_id, result, message
                )
            )
        
        # Record usage statistics
        await self._record_agent_usage(agent_model.id, True, len(tools_used))
        
        return response
        
    except Exception as e:
        logger.error(f"Error processing message with agent: {e}")
        await self._record_agent_usage(agent_model.id, False)
        return self._create_error_response()

async def _store_conversation_result(
    self,
    thread_id: str,
    agent_result,
    user_message: str
) -> None:
    """Store complete conversation result using proper ModelMessage format."""
    try:
        # Get all messages from this run (includes both user and assistant)
        new_messages = agent_result.all_messages()
        
        # Store as JSON using ModelMessagesTypeAdapter
        from app.services.message_converter_service import message_converter_service
        json_string = await message_converter_service.serialize_model_messages_to_json(new_messages)
        
        # Store in database for future retrieval
        async with get_async_db_session() as db:
            message_blob = Message(
                thread_id=UUID(thread_id),
                role="system",
                content=json_string,
                message_type="model_message_blob"  # New message type
            )
            db.add(message_blob)
            await db.commit()
            
    except Exception as e:
        logger.error(f"Failed to store conversation result for thread {thread_id}: {e}")
```

## Step-by-Step Implementation Plan

> **Critical**: Each step must be completed with all tests passing and no Docker log errors before proceeding to the next step.

### Step 1: Create Message Converter Service
**Objective**: Build proper ModelMessage conversion infrastructure

#### 1.1 Install Required Dependencies
```bash
# Check if pydantic-ai is latest version
poetry show pydantic-ai

# Install/update if needed
poetry add "pydantic-ai>=0.4.11"
```

#### 1.2 Create Message Converter Service
Create `app/services/message_converter_service.py` with complete implementation above.

#### 1.3 Create Unit Tests
Create `tests/unit/services/test_message_converter_service.py`:

```python
import pytest
import uuid
import json
from datetime import datetime, timezone
from unittest.mock import MagicMock
from pydantic_ai.messages import ModelMessage, ModelRequest, ModelResponse, UserPromptPart, TextPart
from app.services.message_converter_service import message_converter_service, ModelMessagesTypeAdapter
from app.models.chat import Message

@pytest.mark.asyncio
async def test_convert_db_messages_to_model_messages():
    """Test converting database messages to ModelMessage format."""
    
    # Create mock database messages
    user_msg = MagicMock()
    user_msg.role = "user"
    user_msg.content = "Hello, how are you?"
    user_msg.created_at = datetime.now(timezone.utc)
    
    assistant_msg = MagicMock()
    assistant_msg.role = "assistant"  
    assistant_msg.content = "I'm doing well, thanks for asking!"
    assistant_msg.created_at = datetime.now(timezone.utc)
    
    db_messages = [user_msg, assistant_msg]
    
    # Convert to ModelMessage format
    model_messages = await message_converter_service.convert_db_messages_to_model_messages(db_messages)
    
    # Verify results
    assert len(model_messages) == 2
    assert isinstance(model_messages[0], ModelRequest)
    assert isinstance(model_messages[1], ModelResponse)
    
    # Check user message
    user_part = model_messages[0].parts[0]
    assert isinstance(user_part, UserPromptPart)
    assert user_part.content == "Hello, how are you?"
    
    # Check assistant message
    assistant_part = model_messages[1].parts[0]
    assert isinstance(assistant_part, TextPart)
    assert assistant_part.content == "I'm doing well, thanks for asking!"

@pytest.mark.asyncio
async def test_serialize_model_messages_to_json():
    """Test serializing ModelMessages to JSON."""
    
    # Create test ModelMessage objects
    model_request = ModelRequest(
        parts=[
            UserPromptPart(
                content="Test message",
                timestamp=datetime.now(timezone.utc)
            )
        ]
    )
    
    model_response = ModelResponse(
        parts=[TextPart(content="Test response")],
        usage=RequestUsage(input_tokens=5, output_tokens=10),
        model_name="gpt-4o-mini",
        timestamp=datetime.now(timezone.utc)
    )
    
    model_messages = [model_request, model_response]
    
    # Serialize to JSON
    json_string = await message_converter_service.serialize_model_messages_to_json(model_messages)
    
    # Verify JSON is valid
    assert isinstance(json_string, str)
    json_data = json.loads(json_string)
    assert isinstance(json_data, list)
    assert len(json_data) == 2

@pytest.mark.asyncio
async def test_deserialize_json_to_model_messages():
    """Test deserializing JSON back to ModelMessages."""
    
    # Create test data and serialize
    model_messages = [
        ModelRequest(
            parts=[UserPromptPart(content="Test", timestamp=datetime.now(timezone.utc))]
        )
    ]
    
    json_string = await message_converter_service.serialize_model_messages_to_json(model_messages)
    
    # Deserialize back
    deserialized = await message_converter_service.deserialize_json_to_model_messages(json_string)
    
    # Verify deserialization
    assert len(deserialized) == 1
    assert isinstance(deserialized[0], ModelRequest)
    assert deserialized[0].parts[0].content == "Test"

@pytest.mark.asyncio
async def test_convert_empty_messages():
    """Test handling empty message lists."""
    
    db_messages = []
    model_messages = await message_converter_service.convert_db_messages_to_model_messages(db_messages)
    
    assert model_messages == []

@pytest.mark.asyncio
async def test_convert_invalid_role():
    """Test handling messages with invalid roles."""
    
    invalid_msg = MagicMock()
    invalid_msg.role = "system"  # Unsupported role
    invalid_msg.content = "System message"
    invalid_msg.created_at = datetime.now(timezone.utc)
    
    db_messages = [invalid_msg]
    model_messages = await message_converter_service.convert_db_messages_to_model_messages(db_messages)
    
    # Should skip invalid roles
    assert len(model_messages) == 0
```

#### 1.4 Validation Commands
```bash
# Test service creation and imports
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run python -c "
from app.services.message_converter_service import message_converter_service, ModelMessagesTypeAdapter
print('‚úÖ Message Converter Service imports successfully')
"

# Run unit tests
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run pytest tests/unit/services/test_message_converter_service.py -v

# Check for any import errors
docker compose logs app | grep -i "import\|module\|error"
```

**Success Criteria**:
- [ ] All unit tests pass
- [ ] Service imports without errors
- [ ] ModelMessagesTypeAdapter works correctly
- [ ] No Docker container errors

---

### Step 2: Enhanced AI Chat Service with ModelMessage Support
**Objective**: Update AI Chat Service to return proper ModelMessage format

#### 2.1 Update AI Chat Service
Add new method `get_thread_history_as_model_messages()` to `app/services/ai_chat_service.py` with implementation above.

#### 2.2 Create Unit Tests
Create `tests/unit/services/test_ai_chat_service_model_messages.py`:

```python
import pytest
import uuid
from datetime import datetime, timezone
from unittest.mock import AsyncMock, MagicMock, patch
from pydantic_ai.messages import ModelMessage, ModelRequest, UserPromptPart
from app.services.ai_chat_service import ai_chat_service

@pytest.mark.asyncio
async def test_get_thread_history_as_model_messages():
    """Test retrieving thread history as ModelMessage format."""
    
    thread_id = str(uuid.uuid4())
    user_id = "test_user"
    agent_id = str(uuid.uuid4())
    
    with patch('app.database.get_async_db_session') as mock_db_session, \
         patch('app.services.ai_chat_service.message_converter_service') as mock_converter:
        
        # Mock database setup
        mock_db = AsyncMock()
        mock_db_session.return_value.__aenter__.return_value = mock_db
        
        # Mock thread verification
        thread_mock = MagicMock()
        thread_mock.id = uuid.UUID(thread_id)
        thread_result = MagicMock()
        thread_result.scalar_one_or_none.return_value = thread_mock
        
        # Mock messages
        message1 = MagicMock()
        message1.role = "user"
        message1.content = "Hello"
        message1.created_at = datetime.now(timezone.utc)
        
        messages_result = MagicMock()
        messages_result.scalars.return_value.all.return_value = [message1]
        
        # Configure mock to return thread first, then messages
        mock_db.execute.side_effect = [thread_result, messages_result]
        
        # Mock converter service
        mock_model_message = ModelRequest(
            parts=[UserPromptPart(content="Hello", timestamp=datetime.now(timezone.utc))]
        )
        mock_converter.convert_db_messages_to_model_messages.return_value = [mock_model_message]
        
        # Test the method
        history = await ai_chat_service.get_thread_history_as_model_messages(
            thread_id=thread_id,
            user_id=user_id,
            agent_id=agent_id,
            use_memory_context=True
        )
        
        # Verify results
        assert len(history) == 1
        assert isinstance(history[0], ModelRequest)
        mock_converter.convert_db_messages_to_model_messages.assert_called_once_with([message1])

@pytest.mark.asyncio
async def test_get_thread_history_model_messages_memory_disabled():
    """Test that empty list is returned when memory is disabled."""
    
    thread_id = str(uuid.uuid4())
    user_id = "test_user"
    agent_id = str(uuid.uuid4())
    
    history = await ai_chat_service.get_thread_history_as_model_messages(
        thread_id=thread_id,
        user_id=user_id,
        agent_id=agent_id,
        use_memory_context=False
    )
    
    assert history == []
```

#### 2.3 Validation Commands
```bash
# Test enhanced AI chat service
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run python -c "
from app.services.ai_chat_service import ai_chat_service
print('‚úÖ Enhanced AI Chat Service imports successfully')
"

# Run unit tests
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run pytest tests/unit/services/test_ai_chat_service_model_messages.py -v

# Check for errors
docker compose logs app | grep -i error
```

**Success Criteria**:
- [ ] Enhanced method works correctly
- [ ] Unit tests pass
- [ ] ModelMessage format is returned
- [ ] No Docker container errors

---

### Step 3: Updated Dynamic Agent Factory with ModelMessage Format
**Objective**: Fix Dynamic Agent Factory to use proper ModelMessage format

#### 3.1 Update Dynamic Agent Factory
Update `process_message_with_agent` method in `app/services/dynamic_agent_factory.py` with implementation above.

#### 3.2 Add Required Imports
```python
from typing import List
from pydantic_ai.messages import ModelMessage
```

#### 3.3 Create Unit Tests
Create `tests/unit/services/test_dynamic_agent_factory_model_messages.py`:

```python
import pytest
import uuid
from unittest.mock import AsyncMock, MagicMock, patch
from pydantic_ai.messages import ModelMessage, ModelRequest, UserPromptPart
from app.services.dynamic_agent_factory import dynamic_agent_factory
from app.schemas.ai_response import ChatResponse, CustomerSupportContext

@pytest.mark.asyncio
async def test_process_message_with_model_message_history():
    """Test processing message with proper ModelMessage format."""
    
    # Mock agent model
    agent_model = MagicMock()
    agent_model.id = uuid.uuid4()
    agent_model.use_memory_context = True
    agent_model.max_context_size = 1000
    agent_model.max_iterations = 5
    
    thread_id = str(uuid.uuid4())
    
    # Mock ModelMessage history
    mock_model_message = ModelRequest(
        parts=[UserPromptPart(content="Previous question", timestamp=datetime.now(timezone.utc))]
    )
    mock_history = [mock_model_message]
    
    with patch.object(dynamic_agent_factory, 'create_agent_from_model') as mock_create, \
         patch('app.services.ai_chat_service.ai_chat_service') as mock_chat_service:
        
        # Mock Pydantic AI agent
        mock_agent = AsyncMock()
        mock_result = MagicMock()
        mock_result.output = ChatResponse(
            content="Response with ModelMessage context",
            confidence=0.9,
            requires_escalation=False,
            tools_used=[]
        )
        mock_agent.run.return_value = mock_result
        mock_create.return_value = mock_agent
        
        # Mock chat service to return ModelMessage format
        mock_chat_service.get_thread_history_as_model_messages = AsyncMock(return_value=mock_history)
        
        # Create context
        context = CustomerSupportContext(
            user_input="Follow up question",
            user_metadata={"user_id": "test_user"}
        )
        
        # Process message
        response = await dynamic_agent_factory.process_message_with_agent(
            agent_model=agent_model,
            message="Follow up question",
            context=context,
            thread_id=thread_id
        )
        
        # Verify ModelMessage format was used
        mock_agent.run.assert_called_once()
        call_args = mock_agent.run.call_args
        
        # Should be called with ModelMessage history
        assert 'message_history' in call_args.kwargs
        history_param = call_args.kwargs['message_history']
        assert len(history_param) == 1
        assert isinstance(history_param[0], ModelRequest)
        assert response.content == "Response with ModelMessage context"
```

#### 3.4 Validation Commands
```bash
# Test enhanced dynamic agent factory
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run python -c "
from app.services.dynamic_agent_factory import dynamic_agent_factory
from app.services.message_converter_service import message_converter_service
print('‚úÖ All services import successfully')
"

# Run new tests
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run pytest tests/unit/services/test_dynamic_agent_factory_model_messages.py -v

# Ensure existing functionality still works
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run pytest tests/unit/services/test_dynamic_agent_factory_history.py -v

# Check for errors
docker compose logs app | grep -i error
```

**Success Criteria**:
- [ ] All new tests pass
- [ ] ModelMessage format is properly used
- [ ] Services import correctly
- [ ] No Docker errors
- [ ] Conversation context works properly

---

### Step 4: Message History Storage Enhancement
**Objective**: Store complete ModelMessage conversation history

#### 4.1 Update Database Schema (Optional)
Add new message type to distinguish ModelMessage blobs:

```python
# In app/models/chat.py - add to Message model if not exists
message_type = Column(
    String(50),
    nullable=True,
    default="user_message",
    comment="Type: user_message, assistant_message, model_message_blob"
)
```

#### 4.2 Create Integration Test
Create `tests/integration/test_model_message_integration.py`:

```python
import pytest
import uuid
from datetime import datetime, timezone
from unittest.mock import AsyncMock, MagicMock, patch
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic_ai.messages import ModelMessage

from app.models.chat import Thread, Message
from app.models.ai_agent import Agent
from app.models.organization import Organization
from app.services.dynamic_agent_factory import dynamic_agent_factory
from app.services.ai_chat_service import ai_chat_service
from app.services.message_converter_service import message_converter_service
from app.schemas.ai_response import ChatResponse, CustomerSupportContext

@pytest.mark.asyncio
async def test_complete_model_message_flow():
    """Test complete flow with proper ModelMessage format."""
    
    # This test would verify that:
    # 1. Database messages convert to ModelMessage format
    # 2. ModelMessage format is passed to agent.run()
    # 3. Agent result is stored properly for future history
    # 4. Conversation continuity works across multiple exchanges
    
    # Implementation would create real database objects,
    # test the conversion, and verify proper ModelMessage usage
    pass  # Placeholder for full implementation
```

#### 4.3 Validation Commands
```bash
# Run integration tests
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run pytest tests/integration/test_model_message_integration.py -v

# Test auth flow with new implementation
PYTHONPATH=/Users/aristotle/projects/support-extension poetry run python test_auth_flow.py

# Check for any errors
docker compose logs app | grep -A 5 -B 5 "error\|Error\|ERROR" | tail -20
```

**Success Criteria**:
- [ ] Integration tests pass
- [ ] Auth flow test shows proper conversation continuity
- [ ] Second messages no longer fail
- [ ] ModelMessage format is used throughout
- [ ] No critical Docker errors

---

## Final Validation Checklist

### ‚úÖ Proper Pydantic AI ModelMessage Integration
- [ ] Uses ModelMessage format (ModelRequest/ModelResponse) instead of simple dicts
- [ ] Implements ModelMessagesTypeAdapter for serialization/deserialization
- [ ] Includes proper message parts (UserPromptPart, TextPart)
- [ ] Stores complete conversation history using result.all_messages()

### ‚úÖ Core Functionality
- [ ] Message conversion from database format to ModelMessage works
- [ ] Context size limits applied to ModelMessage objects
- [ ] Memory context can be enabled/disabled per agent
- [ ] Conversation continuity maintained across message exchanges

### ‚úÖ Testing Coverage
- [ ] Unit tests for message converter service
- [ ] Unit tests for enhanced AI chat service
- [ ] Unit tests for updated dynamic agent factory
- [ ] Integration tests covering complete ModelMessage flow

### ‚úÖ System Health
- [ ] Auth flow test passes completely (both messages work)
- [ ] API endpoints remain healthy
- [ ] No import errors or circular dependencies
- [ ] Existing functionality preserved

## Critical Differences from Previous Implementation

| Aspect | Previous (Incorrect) | New (Correct) |
|--------|---------------------|---------------|
| Message Format | `List[Dict[str, str]]` | `List[ModelMessage]` |
| Message Parts | Simple dict with role/content | UserPromptPart, TextPart |
| Serialization | Custom JSON | ModelMessagesTypeAdapter |
| Agent Usage | Dict format in message_history | Proper ModelMessage format |
| Conversation Structure | Flat message list | Structured Request/Response pairs |
| Time Handling | ISO string timestamps | datetime objects |

## Key Benefits of Proper Implementation

1. **üéØ Correct Pydantic AI Integration**: Uses official ModelMessage format exactly as intended
2. **üìù Rich Message Structure**: Supports proper conversation structure with parts and metadata
3. **üîÑ Conversation Continuity**: Enables true conversation context across multiple exchanges
4. **‚ö° Performance Optimized**: Leverages Pydantic AI's efficient message handling
5. **üîß Extensible Design**: Supports future enhancements like tool calls, attachments, etc.
6. **üéÅ Future-Proof**: Aligns with Pydantic AI's design patterns for long-term compatibility

This implementation fixes the fundamental message format issue and ensures proper conversation history functionality with Pydantic AI agents.